#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
# Learn about this object by saying ?summary.lm and by saying str(d)
cat("OLS gave slope of ", d1$coefficients[2,1],
"and an R-sqr of ", d1$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1, 1) # plot one diagnostic graphs
#========================Part(2)=====================================================
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#(Q6)Now, change the coefficient of the non-linear term in the original model for (A) training
#(Q6)Now, change the coefficient of the non-linear term in the original model for (A) training
#and (B) testing to a large value instead. What do you notice about the residual plot?
#=================================Part(3)==================================================
#(Q9) Draw a pair-wise scatter plot between Lung Capacity, Age and Height.
#(Q10) Calculate correlation between Age and LungCap, and between Height and LungCap.
#(Q11) Which of the two input variables (Age, Height) are more correlated to the
#(Q13) Fit a liner regression model where the dependent variable is LungCap
#(Q16) Show the coefficients of the linear model. Do they make sense?
#(Q17) Redraw a scatter plot between Age and LungCap. Display/Overlay the linear model (a line) over it.
#(Q17) Redraw a scatter plot between Age and LungCap. Display/Overlay the linear model (a line) over it.
#Hint: Use the function abline(model, col="red").
#(Q17) Redraw a scatter plot between Age and LungCap. Display/Overlay the linear model (a line) over it.
#Hint: Use the function abline(model, col="red").
#Note (1) : A warning will be displayed that this function will display only the first two
#(Q17) Redraw a scatter plot between Age and LungCap. Display/Overlay the linear model (a line) over it.
#Hint: Use the function abline(model, col="red").
#Note (1) : A warning will be displayed that this function will display only the first two
#           coefficients in the model. It's OK.
#Logit
rm(list=ls())
# [Data Description]:
# the marketing campaign team wants to send
# special offers to those respondents with the highest probability of purchase.
# the response variable is purchase or no purchase
# given customer income and age and product price
Mydata <- read.csv("survey.csv",header=TRUE,sep=",")
# [1] Explore data
table(Mydata$MYDEPV) # the outcome variable
# purchase or no purchase
with(Mydata, table(Price,MYDEPV))
summary(Mydata$Age)
cor.mat <- cor(Mydata[,-1]) # the input variables
cor.mat # Note: The general rule is not to include variables in your model that are
# [2] Test a model with 3 variables Price, Income and Age
mylogit <- glm(MYDEPV ~ Income + Age + as.factor(Price),
data =Mydata, family=binomial(link="logit"),
na.action=na.pass) # as.factor(Price) : to deal with price as categorical feature
summary(mylogit)
#(Q21)Calculate the mean squared error (MSE)of the training data.
#setwd("~/LAB")
rm(list=ls())
#=============================Part(1)=====================================
x <- runif(100, 0, 10)     # 100 draws between 0 & 10
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 0.2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 20)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 0.2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 20)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 0.2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
# Learn about this object by saying ?summary.lm and by saying str(d)
cat("OLS gave slope of ", d1$coefficients[2,1],
"and an R-sqr of ", d1$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 20)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
# Learn about this object by saying ?summary.lm and by saying str(d)
cat("OLS gave slope of ", d1$coefficients[2,1],
"and an R-sqr of ", d1$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1, 1) # plot one diagnostic graphs
#(Q1) Try changing the value of standard deviation (sd) in the next command
#How do the data points change for different values of standard deviation?
y <- 5 + 6*x + rnorm(100, sd = 0.2)  # default values for rnorm (mean = 0 and sigma = 1)
#Plot it
plot (x,y)
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
# Compact model results
print(model1)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
par(mfrow=c(1,1))
plot(y,y, type="l", xlab="true y", ylab="predicted y") # ploting the ideal line
points(y, ypred) # plotting the predicted points
# Detailed model results
d1 <- summary(model1)
print(model1)
# Learn about this object by saying ?summary.lm and by saying str(d)
cat("OLS gave slope of ", d1$coefficients[2,1],
"and an R-sqr of ", d1$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1, 1) # plot one diagnostic graphs
#========================Part(2)=====================================================
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 10*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 10*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 100*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 100*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 10*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 10*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#=================================Part(3)==================================================
#(Q7) Import the dataset LungCapData.tsv. What are the variables in this dataset?
lung_data <- read.table("LungCapData.tsv", header = TRUE, sep = "\t")
#=================================Part(3)==================================================
#(Q7) Import the dataset LungCapData.tsv. What are the variables in this dataset?
setwd("E:\CMP4\LAAAASSSTT TTTEEEERRRMM\Big Data\Big-Data-Labs\Lab5\Linear Regression Requirement")
lung_data <- read.table("LungCapData.tsv", header = TRUE, sep = "\t")
#=================================Part(3)==================================================
#(Q7) Import the dataset LungCapData.tsv. What are the variables in this dataset?
setwd("E:\CMP4\LAAAASSSTT TTTEEEERRRMM\Big Data\Big-Data-Labs\Lab5\Linear Regression Requirement")
#=================================Part(3)==================================================
#(Q7) Import the dataset LungCapData.tsv. What are the variables in this dataset?
setwd("E:/CMP4/LAAAASSSTT TTTEEEERRRMM/Big Data/Big-Data-Labs/Lab5/Linear Regression Requirement")
# Import the dataset
lung_data <- read.table("LungCapData.tsv", header = TRUE, sep = "\t")
# View the variables in the dataset
names(lung_data)
plot(lung_data$Age, lung_data$LungCap, xlab = "Age", ylab = "LungCap", main = "Age vs. LungCap Scatter Plot")
# Create a pair-wise scatter plot
pairs(lung_data[, c("LungCap", "Age", "Height")], main = "Pair-wise Scatter Plot")
# Calculate the correlation between Age and LungCap
cor_age_lungcap <- cor(lung_data$Age, lung_data$LungCap)
# Calculate the correlation between Age and LungCap
cor_age_lungcap <- cor(lung_data$Age, lung_data$LungCap)
# Calculate the correlation between Age and LungCap
cor_age_lungcap <- cor(lung_data$Age, lung_data$LungCap)
# Calculate the correlation between Height and LungCap
cor_height_lungcap <- cor(lung_data$Height, lung_data$LungCap)
# Print the correlations
cat("Correlation between Age and LungCap:", cor_age_lungcap, "\n")
cat("Correlation between Height and LungCap:", cor_height_lungcap, "\n")
model <- lm(LungCap ~ Age + Height + Smoke + Gender + Caesarean, data = lung_data)
#(Q14) Show a summary of this model
summary(model)
#(Q17) Redraw a scatter plot between Age and LungCap. Display/Overlay the linear model (a line) over it.
#Hint: Use the function abline(model, col="red").
#Note (1) : A warning will be displayed that this function will display only the first two
#           coefficients in the model. It's OK.
#Note (2) : If you are working correctly, the line will not be displayed on the plot. Why?
# Scatter plot of Age vs. LungCap
plot(lung_data$Age, lung_data$LungCap, xlab = "Age", ylab = "LungCap", main = "Scatter Plot of Age vs. LungCap")
# Overlay the linear model
abline(model, col = "red")
# Overlay the linear model
abline(model, col = "red")
#(Q18)Repeat Q13 but with these variables Age, Smoke and Cesarean as the only independent variables.
# Fit a linear regression model with Age, Smoke, and Cesarean as independent variables
model2 <- lm(LungCap ~ Age + Smoke + Caesarean, data = lung_data)
# Show a summary of the new model
summary(model2)
#(Q20)Predict results for this regression line on the training data.
# Predict LungCap values using the new model
predicted_values <- predict(model2, lung_data)
# Show the first few predicted values
head(predicted_values)
#(Q21)Calculate the mean squared error (MSE)of the training data.
# Calculate the residuals (actual - predicted) from the model
residuals <- lung_data$LungCap - predicted_values
# Calculate the Mean Squared Error (MSE)
mse <- mean(residuals^2)
mse
# Show the first few predicted values
predicted_values
)
# Show the first few predicted values
head(predicted_values)
#(Q19)Repeat Q16, Q17 for the new model. What happened?
# Scatter plot of Age vs. LungCap
plot(lung_data$Age, lung_data$LungCap, xlab = "Age", ylab = "LungCap", main = "Scatter Plot of Age vs. LungCap")
# Overlay the linear model
abline(model2, col = "red")
# [Data Description]:
# the marketing campaign team wants to send
# special offers to those respondents with the highest probability of purchase.
# the response variable is purchase or no purchase
# given customer income and age and product price
Mydata <- read.csv("survey.csv",header=TRUE,sep=",")
# [Data Description]:
# the marketing campaign team wants to send
# special offers to those respondents with the highest probability of purchase.
# the response variable is purchase or no purchase
# given customer income and age and product price
setwd("E:/CMP4/LAAAASSSTT TTTEEEERRRMM/Big Data/Big-Data-Labs/Lab5/Linear Regression Requirement")
Mydata <- read.csv("survey.csv",header=TRUE,sep=",")
# [Data Description]:
# the marketing campaign team wants to send
# special offers to those respondents with the highest probability of purchase.
# the response variable is purchase or no purchase
# given customer income and age and product price
setwd("E:/CMP4/LAAAASSSTT TTTEEEERRRMM/Big Data/Big-Data-Labs/Lab5/Logistic Regression Requirement")
Mydata <- read.csv("survey.csv",header=TRUE,sep=",")
# [1] Explore data
table(Mydata$MYDEPV) # the outcome variable
# purchase or no purchase
with(Mydata, table(Price,MYDEPV))
summary(Mydata$Age)
cor.mat <- cor(Mydata[,-1]) # the input variables
cor.mat # Note: The general rule is not to include variables in your model that are
# [2] Test a model with 3 variables Price, Income and Age
mylogit <- glm(MYDEPV ~ Income + Age + as.factor(Price),
data =Mydata, family=binomial(link="logit"),
na.action=na.pass) # as.factor(Price) : to deal with price as categorical feature
summary(mylogit)
# [3] ROC Curve
if(!require("ROCR"))
{
install.packages("ROCR")
library(ROCR)
}
#### NOTE: For this part, you need to search and read about the ROC curve.
pred = predict(mylogit, type="response") # this returns the probability scores on the training data
predObj = prediction(pred, Mydata$MYDEPV) # prediction object needed by ROCR
rocObj = performance(predObj, measure="tpr", x.measure="fpr")  # creates ROC curve obj
aucObj = performance(predObj, measure="auc")  # auc object
# [3] ROC Curve
if(!require("ROCR"))
{
install.packages("ROCR", repos = "https://cloud.r-project.org/")
library(ROCR)
}
# [3] ROC Curve
if(!require("ROCR"))
{
install.packages("ROCR", repos = "https://cloud.r-project.org/")
library(ROCR)
}
#### NOTE: For this part, you need to search and read about the ROC curve.
pred = predict(mylogit, type="response") # this returns the probability scores on the training data
predObj = prediction(pred, Mydata$MYDEPV) # prediction object needed by ROCR
rocObj = performance(predObj, measure="tpr", x.measure="fpr")  # creates ROC curve obj
aucObj = performance(predObj, measure="auc")  # auc object
auc = aucObj@y.values[[1]]
auc   # the auc score: tells you how well the model predicts.
# plot the roc curve
plot(rocObj, main = paste("Area under the curve:", auc))
# [4] Predictions
#Prediction - 1
Price <- c(10,20,30)
Age <- c(mean(Mydata$Age))
Income <- c(mean(Mydata$Income))
newdata1 <- data.frame(Income,Age,Price) # Note: The predict function requires the variables to be named exactly as in the fitted model.
newdata1
newdata1$PurchaseP <- predict (mylogit,newdata=newdata1,type="response")
newdata1
#Prediction - 2
newdata2 <- data.frame(Age=seq(min(Mydata$Age),max(Mydata$Age),2),
Income=mean(Mydata$Income),Price=30)
newdata2
newdata2$PurchaseP <- predict(mylogit,newdata=newdata2,type="response")
newdata2
cbind(newdata2$Age,newdata2$PurchaseP)
plot(newdata2$Age,newdata2$PurchaseP)
#Prediction - 3
newdata3 <- data.frame(Income= seq(20,90,10),Age=mean(Mydata$Age),Price=30)
newdata3$PurchaseP<-predict(mylogit,newdata=newdata3,type="response")
cbind(newdata3$Income,newdata3$PurchaseP)
plot(newdata3$Income,newdata3$PurchaseP)
#Prediction 4
newdata4 <- data.frame (Age= round(runif(10,min(Mydata$Age),max(Mydata$Age))),
Income= round(runif(10,min(Mydata$Income),max(Mydata$Income))),
Price = round((runif(10,10,30)/10))*10)
newdata4$Prob <- predict(mylogit,newdata=newdata4,type="response")
newdata4
